<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Projects on Data Science and Machine Learning Portfolio</title>
    <link>http://localhost:1313/post/</link>
    <description>Recent content in Projects on Data Science and Machine Learning Portfolio</description>
    <generator>Hugo</generator>
    <language>en-US</language>
    <lastBuildDate>Mon, 10 Apr 2017 11:00:59 -0400</lastBuildDate>
    <atom:link href="http://localhost:1313/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Detecting credit card fraud</title>
      <link>http://localhost:1313/post/creditcardfraud/</link>
      <pubDate>Mon, 10 Apr 2017 11:00:59 -0400</pubDate>
      <guid>http://localhost:1313/post/creditcardfraud/</guid>
      <description></description>
    </item>
    <item>
      <title>The Matrix - GPT</title>
      <link>http://localhost:1313/post/matrix-gpt/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/post/matrix-gpt/</guid>
      <description>&lt;p&gt;Matrix GPT&lt;/p&gt;&#xA;&lt;p&gt;Overview&lt;/p&gt;&#xA;&lt;p&gt;Matrix GPT is a transformer-based GPT model implemented from scratch using PyTorch, inspired by the seminal paper &amp;ldquo;Attention is All You Need&amp;rdquo; and Andrej Karpathy’s &amp;ldquo;Let’s Build GPT&amp;rdquo; YouTube series. The model is trained on the scripts of The Matrix and The Matrix Reloaded, aiming to generate original text in the style of these iconic films. This project showcases the application of transformer architecture to a creative text generation task, with model weights to be shared once sufficient training is completed.&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
